Metadata-Version: 2.4
Name: model-scout
Version: 0.1.0
Summary: Automatic model benchmarking for sequence-to-property prediction
Author-email: Juan Irizarry-Cole <your.email@example.com>
License: MIT
Project-URL: Homepage, https://github.com/icyjayd/model-scout
Project-URL: Issues, https://github.com/icyjayd/model-scout/issues
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: numpy>=1.23
Requires-Dist: pandas>=1.5
Requires-Dist: scikit-learn>=1.2
Requires-Dist: scipy>=1.10
Requires-Dist: matplotlib<4.0.0,>=3.10.7
Requires-Dist: seaborn<0.14.0,>=0.13.2
Requires-Dist: xgboost<4.0.0,>=3.1.1
Requires-Dist: lightgbm<5.0.0,>=4.6.0
Requires-Dist: tqdm<5.0.0,>=4.67.1
Provides-Extra: boost
Requires-Dist: xgboost>=2.0; extra == "boost"
Requires-Dist: lightgbm>=4.0; extra == "boost"
Provides-Extra: bio
Requires-Dist: torch>=2.2; extra == "bio"
Requires-Dist: esm>=2.0; extra == "bio"


# ğŸ§  Model Scout  
**Automatic model benchmarking for sequenceâ€“property prediction**

Model Scout is a lightweight but extensible toolkit for **benchmarking machine learning models** on sequence-based datasets â€” such as protein, DNA, or other biological sequence data.  
It automates testing of modelâ€“encodingâ€“sample-size combinations to help researchers quickly identify which ML configurations perform best for a given prediction task.

---

## ğŸš€ Features

| Category | Description |
|-----------|--------------|
| ğŸ”¬ **Multi-model benchmarking** | Run batteries of regression or classification models with one command. |
| ğŸ§¬ **Sequence encodings** | Supports one-hot, k-mer, and AAC (amino acid composition) encodings. |
| âš™ï¸ **Auto-sampling** | Test models on different dataset sizes (`--n-samples`) to estimate scalability. |
| ğŸ“ˆ **Spearman analysis** | Automatically computes Spearman Ï and p-values, with graceful handling of constant inputs. |
| ğŸ§© **Parallel execution** | Run many experiments in parallel via `--jobs`. |
| ğŸ“Š **Built-in visualization** | Generates performance and runtime plots automatically. |
| ğŸ§  **Interoperable** | Can be used as a command-line tool or Python library. |

---

## ğŸ§© Quick Start

### 1ï¸âƒ£ Installation

You can install `model-scout` directly from your local clone (recommended for development):

```bash
pip install -e .
```

Or, if you use **Poetry**:

```bash
poetry install
```

---

### 2ï¸âƒ£ Basic usage

Run a minimal benchmark on your dataset:

```bash
model-scout "data/GB1_sequences.npy" --labels "data/GB1_labels.npy" --models rf ridge --encodings aac --n-samples 500 --jobs 2
```

**Arguments:**

| Flag | Description |
|------|--------------|
| `--labels` | Path to the labels file (`.npy` or `.csv`). |
| `--models` | Which models to test (e.g., `rf ridge svr logreg svm gb`). |
| `--encodings` | Encoding type: `aac`, `kmer`, or `onehot`. |
| `--n-samples` | Number of samples per run (integer or `all`). |
| `--jobs` | Parallel jobs for multi-model runs. |
| `--outdir` | Optional: specify output directory for runs. |

---

## ğŸ§¬ Example full workflow

Run all regression models with two encodings and multiple sample sizes:

```bash
model-scout data/GB1_sequences.npy --labels data/GB1_labels.npy   --models rf ridge svr gb lasso   --encodings aac kmer   --n-samples 500 2000 all   --jobs 4
```

Youâ€™ll get automatic:
- Metrics table (`results.csv`)  
- Performance plots (Spearman Ï, RÂ², RMSE)  
- Runtime summary (`runtime.csv`)  
- A timestamped results folder under `runs/model-scout/`

---

## ğŸ§° Internals Overview

### ğŸ§© Core Modules

| Module | Purpose |
|---------|----------|
| `data.py` | Handles data loading from `.csv` or `.npy`, with sequence normalization. |
| `encoding.py` | Provides encoding utilities (AAC, k-mer, one-hot). |
| `models.py` | Defines model registry and builder (Ridge, Lasso, SVR, Random Forest, Gradient Boosting, etc.). |
| `training.py` | Orchestrates model training, evaluation, and metric calculation. |
| `metrics.py` | Implements Spearman correlation and regression metrics safely. |
| `plotting.py` | Generates performance and runtime plots using Matplotlib and Seaborn. |
| `run_single.py` | Runs a single benchmark configuration programmatically. |
| `main.py` | CLI entry point for batch benchmarking (what `model-scout` calls). |

---

## ğŸ“ˆ Outputs

When you run `model-scout`, outputs are stored under:

```
runs/model-scout/<timestamp>/
â”œâ”€â”€ results.csv            # Spearman, RÂ², RMSE, p-values
â”œâ”€â”€ runtime.csv            # Timing summary
â”œâ”€â”€ plots/
â”‚   â”œâ”€â”€ spearman.png
â”‚   â”œâ”€â”€ runtime.png
â”‚   â””â”€â”€ ...
â””â”€â”€ log.txt                # Full execution log
```

---

## ğŸ§  Example in Python

You can also use Model Scout as a library:

```python
from model_scout.run_single import run_single

run_single(
    seq_file="data/GB1_sequences.npy",
    label_file="data/GB1_labels.npy",
    model_name="rf",
    encoding="aac",
    n_samples=500
)
```

This returns a metrics dictionary and automatically writes results under `/runs/model-scout/`.

---

## âš™ï¸ Encodings Supported

| Encoding | Description | Notes |
|-----------|--------------|-------|
| `aac` | Amino acid composition (20-length vector). | Fastest, low information loss. |
| `kmer` | Frequency of k-length substrings (default `k=3`). | Richer representation, slower. |
| `onehot` | 2D binary matrix per sequence. | High-dimensional, suitable for deep models. |

Encodings are cached to disk in `/cache/<encoding>/` to avoid recomputation.

---

## ğŸ§  Model Zoo

| Model | Type | Description |
|--------|------|--------------|
| `ridge` | Regression | Ridge regression (L2-regularized). |
| `lasso` | Regression | Lasso regression (L1-regularized). |
| `svr` | Regression | Support Vector Regressor (RBF kernel). |
| `rf` | Regression | Random Forest Regressor. |
| `gb` | Regression | Gradient Boosting Regressor. |
| `mlp` | Regression | Multi-layer Perceptron (neural network). |
| `logreg` | Classification | Logistic regression. |
| `svm` | Classification | Support Vector Classifier. |
| `xgb`, `lgbm` | Regression/Classification | Optional: enable via `[boost]` extras in `pyproject.toml`. |

---

## âš¡ Performance Tips

- Use smaller `--n-samples` (e.g., 500â€“2000) for quick exploration.  
- Increase `--jobs` to parallelize across CPU cores.  
- Cache is automatic; reruns with the same settings will be nearly instant.  
- Use `--outdir` for organized experiment tracking.

---

## ğŸ§© Development

Install for development with all extras:
```bash
pip install -e .[boost,bio]
```

Run all tests:
```bash
pytest tests -v
```

---

## ğŸ§­ Roadmap

- [ ] Add transformer-based encodings (e.g., ESM embeddings).  
- [ ] Implement hyperparameter sweeps per model.  
- [ ] Expand visualization templates (pairplots, heatmaps).  
- [ ] Integrate automated model selection ranking.

---

## ğŸ“œ License
MIT License Â© 2025 Juan Irizarry-Cole
